\chapter{Prolog Programs as Inductive Definitions}

\section{Introduction}
The evaluation of pure Prolog programs is usually formalized as SLD
resolution applied to first-order Horn clauses.  This is how we
described it in our chapter \ref{fol} on First Order Logic.  However,
there is another, perhaps more basic and fundamental way to understand
logic programs by understanding them as providing {\em Inductive
  Definitions} of sets of terms.  In this formulation, the first order
logic of Horn clauses is just one particular interpretation of such
inductively defined sets of terms.

This chapter is a slight diversion to explore this alternative way to
understand (and formalize) logic programs. [DSW: It may be that this
  chapter and chapter should be interchanged, with ID being
  fundamental and FOL being derived.]

We present a new formulation of the foundations of Logic
Programming.  We emphasize that logic programming is fundamentally
programming with inductive definitions.  The form of inductive
definitions then have a natural interpretation as defining least
models of sets of Horn clauses in first-order logic.  In this
formulation logic is not fundamental; the inductive definition is
fundamental, and logic is a derived aspect of the form of the
inductive definition.

In this formulation the evaluation of logic programs is computation of
the set defined by an inductive definition of a particular form.  We
will explore various strategies for evaluating these inductive
definitions, and will see that these correspond to Horn clause proof
strategies in first-order logic.

The contribution of this chapter does not include any new deep
technical results, but rather is a re-orientation of how one thinks
about logic programming.  We provide insights into how logic
programming is not simply first-order logic, but whose semantics
fundamentally includes the notion of minimality, and whose computation
is productively thought of as evaluation of inductively defined sets
rather than first-order theorem proving.  This presentation builds on
and elaborates the foundational contributions of Marc Denecker and his
colleagues \cite{...}.

\section{Motivation}
Logic Programming has from the beginning been understood (and taught)
as being based fundamentally on the Horn Clause subset of first-order
logic; its semantics based on minimal models; and its evaluation
strategy as based on the theorem proof method of SLD resolution.  This
is clearly good, but it does require various restrictions on the
logic, such as the restriction to Herbrand models, and the restriction
to minimal models that is required to handle negation.  These
restrictions complicate the simple elegance of first-order logic.  And
they can (and have) led to confusions among logic programmers between
properties of fixed point logic and traditional first-order logic.

The approach to the foundations of logic programming that is proposed
in this chapter is based not on first-order logic, but on inductive
definitions, with the relation to logic as arising from a particular
correspondence between the particular form of inductive definitions
and first-order models.  In this formulation the minimality of models
is a fundamental property of inductive definitions and not what might
seem as a somewhat ad hoc restriction to minimal models in first-order
logic.  And the correspondence to Herbrand models of first-order logic
is natural and obvious.

We do not avoid any of the difficulties/complications of Logic
Programming - e.g., all the difficulties of handling negation remain -
but we feel that this new approach makes them easier to motivate and
understand.

\section{Informal Review of Inductive Definitions}

We are interested in inductively defined {\em sets}.  An inductively
defined set is a subset of a carrier set generated by a set of
operators.

An operator on a carrier set $D$ is an element of $\mathcal{F}(D)
\times D$, where $\mathcal{F}(D)$ denotes the set of finite subsets of
$D$.  Elements of the first component of an operator are called {\em
  antecedents} of the operator, and the second component is called the
       {\em consequent}.

An operator $Op = \langle A, d\rangle$, for $A \subseteq D$, applied
to a set $S \subseteq D$, denoted $Op(S)$, is $S \cup \{d\}$ if
$A \subseteq S$, and is $S$ otherwise.

A set of operators $Ops$ applied to a set $S$, denoted $Ops(S)$, is
$\bigcup_{Op\in Ops} Op(S)$.

A set $S \subseteq D$ is closed under an operator, $Op$, if $Op(S)=S$.
A set is closed under a set of operators if it is closed under every
operator in the set, i.e., $Ops(S)=S$.

\begin{definition}{Inductively defined set}
The set inductively defined by a set of operators is the smallest set
closed under the operators.
\end{definition}

A very simple example is the set of even natural numbers.  The carrier
set is the natural numbers $N$.  The operators are $\{\langle \{n\},
n+2\rangle: n \in N\} \cup \{\langle \emptyset, 0 \rangle\}$.  Then
the even natural numbers are the smallest set containing the number
$0$ and closed under the operators.  (How could we inductively define
the set of {\em odd} natural numbers?)

We next define the carrier set for Prolog and describe how pure Prolog
rules determine operators.  The carrier set of interest is the set of
finite, labeled, ordered trees.

\section{Finite, Labeled, Ordered Trees}
\label{trees}
The objects of interest in logic programming are finite, labeled,
ordered trees with leaves of atoms, integers, or floats. (We refer to
them in the following simply as {\em trees}.)  Each internal node is
labeled by an atom, has a finite number of children, and the order of
the children of a node is significant.

We use {\em terms} as a sequential way to represent certain sets of
trees.  A single tree is represented by using a pre-order
traversal, enclosing the sequence of children in parentheses with the
children separated by commas.  Labels are (singly) quoted ASCII
sequences.  Atomic leaves are also quoted ASCII sequences; numeric
leaves are represented as usual in programming languages.

\begin{definition}{Ground Term}
    A {\em ground term} is a string in the following grammar, and
    represents a finite, labeled, ordered tree:
\begin{verbatim}
  term --> integer | float | atom | atom ( term-sequence )
  term-sequence --> term | term , term-sequence
  atom --> ' ascii-seq '
  atom --> unq-atom
  ascii-seq --> ascii | ascii ascii-seq
  unq-atom --> ascii-lc | unq-atom asci-alphanum
\end{verbatim}
The integer and float terms are written as usual in programming
languages, as optionally signed sequences of digits, with a decimal
point for floats.  If an atom starts with a lower-case alpha
character, and contains only alphanumeric characters (and
underscores), then the atom need not be quoted.  The usual convention
is to choose, when convenient, the names of atoms to be unquoted atoms
to make them easier to write and avoid the clutter of extraneous
quotes.
\end{definition}

\begin{example}
  The following are examples of ground terms that represent distinct
  trees:
\begin{verbatim}
    193
    'David S. Warren'
    f(a_bc,12,13.5)
    f(12,13.5,a_bc)
    '+'(5,'*'(7,14))
    cons(a,cons(b,cons(c,nil)))
    employee(george,sales,1988,50000)
\end{verbatim}
\end{example}

We extend the definition of terms to include some that represent
(certain) {\em sets} of trees by introducing ``variables.''
\begin{definition}{Term}
  A {\em term} is a string in the language of the grammar above for {\em
    ground terms} augmented by the following rules:
\begin{verbatim}
  term --> variable
  variable --> ascii-uc | variable ascii-alphanum
\end{verbatim}
\end{definition}
A leaf node of a (non-ground) term tree can be a variable.  So a
variable can appear in the position of any subterm in a term.  (Note
an internal atomic label cannot be replaced by a variable since it is
not a full subterm, but just a piece of one.)  A variable is indicated
by an unquoted sequence of alphanumeric characters (plus the
underscore) that starts with an uppercase alpha character (or
underscore).
\begin{example}
  Examples of non-ground terms:
\begin{verbatim}
  X
  _variable_1
  equal(X,X)
  f(Atom,12,13.5)
  cons(Head,Tail)
  employee(Name,Dept,Birth_Date,Salary)
\end{verbatim}
\end{example}
A term that does {\em not} contain a variable is said to be
``ground.''  A ground term corresponds to a (single) tree.  A term
with variables corresponds to the set of trees that can be obtained
from the term by replacing each variable uniformly with any tree
(i.e., ground term).  ``Uniformly'' means that all occurrences of the
same variable must be replaced with the same tree.  We make this more
precise.

\begin{definition}{Substitution}
A {\em substitution} is a function from some set of variables to
terms.  A {\em ground substitution} is a function from variables to
ground terms.
\end{definition}
\begin{example}
Examples of substitutions are as follows, written as sets of pairs,
called replacements:
\begin{verbatim}
  {X->'David', Y->f(aaa,ZZ)}
  {Name->george, Dept->sales, Birth_Date->1988, Salary->50000}
  {Head->a, Tail->cons(b,cons(c,nil))}
\end{verbatim}
The second and third examples are ground substitutions; the first is
not.
\end{example}

\begin{definition}{Substitution Instance}
  The substitution instance of a term $T$ under substitution $\theta$,
  written $T\theta$, is the result of replacing every variable $V$
  that occurs in $T$ with the term that is the image of $V$ under
  $\theta$.
\end{definition}
\begin{definition}{Ground Instance}
  A {\em ground instance of term $T$} is $T\theta$ for some ground
  substitution $\theta$ such that $T\theta$ is a ground term.
\end{definition}
Examples:
\begin{example}
\begin{verbatim}
  cons(Head,Tail){Head->a, Tail->cons(b,cons(c,nil))} =
    cons(a,cons(b,cons(c,nil)))

  employee(Name,Dept,Birth_Date,Salary)
      {Name->george, Dept->sales, Birth_Date->1988, Salary->50000} =
    employee(george,sales,1988,50000)

  equal(X,X){X->bill,Y->george} = equal(bill,bill)
\end{verbatim}
\end{example}

\section{Inductive Definitions of Sets of Trees}

We next describe pure Prolog rules (including facts) and use them to
define operators on sets of terms.  Such a set of operators will
inductively define a set of terms in the framework described above.

\begin{definition}{Rule}
A {\em rule} is a string in the language of the following grammar:
\begin{verbatim}
  rule --> term .
  rule --> term :- body-terms .
  body-terms -> term | term , body-terms
\end{verbatim}
\end{definition}
The first term of a rule is called the {\em head} of the rule; the
comma-separated sequence of terms after the \verb|:-| symbol is called
the {\em body} of the rule.

\begin{definition}{Inductive Program}
  An {\em inductive program} (or simply {\em program} for short) is a
  sequence of rules, i.e., a string in the language of the following
  grammar:
\begin{verbatim}
  program --> rule | rule program
\end{verbatim}
\end{definition}

\begin{example}
A simple example of a program is:\\
\begin{verbatim}
nil.
cons(aa,Tail) :- Tail.
\end{verbatim}
This is a program with two rules (or clauses).  The first does not
have a body list (and is called a {\em fact}); the second has a body
consisting of a single term.
\end{example}

\begin{definition}{Substitution applied to a rule}
(Needed??)  The result of applying substitution $\theta$ to rule $R$,
  denoted by $R\theta$, is the rule obtained by replacing every
  variable $V$ in $R$ by its image under $\theta$.
\end{definition}

\begin{definition}{Rule Operator}
  A rule $R~=~t$ :- $t_1,t_2,...,t_n$ determines a set of operators:
  for every ground instance of R under a substitution $\theta$, there
  is an operator $\langle \{t_1\theta, t_2\theta, ..., t_n\theta\},
  t\theta\rangle$.
\end{definition}

\begin{definition}{Rule operators from program P}
Let $Ops^P$ be the set of all operators determined by all rules of program $P$.
\end{definition}

\begin{definition}{Meaning of program P, $\bf{M^{least}(P)}$}
  The meaning of a program P is the set inductively defined by its
  rule operators.  I.e., it is the smallest set of trees closed under
  all the operators of $Ops^P$.
\end{definition}

Here we should establish that there exists such a unique set of terms.
This follows from the fact that the intersection of two sets that are
closed under a set of operators is closed under those operators.  Then
we can take the intersection of all such closed sets, which is unique,
and is the smallest closed set.  Note that the set of all trees is
closed under any set of operators, so we have at least one set in the
intersction.

This is a fine definition of the meaning of a program, but it doesn't
suggest a way to compute it.  There will almost certainly be
infinitely many sets closed under the operators of a program, so
generating and intersecting them would not be reasonable.  Fortunately
there are other ways to define this same set.

\begin{definition}{Meaning of a program P, $\bf{M^{bu}(P)}$}
  Let\\
~~~  $M_0(P) = Ops^P(\emptyset)$ and \\
~~~  $M_n(P) = Ops^P(M_{n-1}(P))$.\\
  Then let $M^{bu}(P) = \bigcup_{i=0}^\infty M_i(P)$
\end{definition}

This definition can be used to compute the meaning of a program that
generates a finite number of operators.  We note that $M_i(P)
\subseteq M_{i+1}(P)$ for all $i$.  We first apply all the operators
to the empty set, the apply all the operators to that set, and
continue in this way applying all operators to the previously
generated set obtaining a new set until no new elements are added to
the set.  This means that all the rest of the sets in the infinite
sequence are the same set and we are finished.  Since the number of
operators is finite, we can only add finitely many elements, at most
the consequents of all the operators.  The {\em bu} in the name of the
meaning function stands for ``bottom-up''.  (Do an example.)

We need to prove that $M^{bu}(P) \iff M^{least}(P)$ for all programs
$P$.  (This is generally true for any set of operators on any carrier
set.)

However, before we prove this, we give yet another definition of this
same set, one that uses proof sequences.

\begin{definition}{Proof Sequence}
  Given a set of operators $Ops$ on a carrier set $D$, a {\em proof
    sequence for $d_n \in D$ from $Ops$} is a finite sequence of
  elements of $D$, $\langle d_1, d_2, ...., d_n \rangle$, such that
  for every $d_i$ in the sequence, there is an operator $\langle A,
  d_i \rangle$ such that $A \subseteq \{d_1, d_2, ..., d_{i-1}\}$.
  I.e., all elements in the antecedent of the operator appear in the
  sequence before $d_i$.  Note that the consequent of an operator with
  an empty antecedent may appear anywhere in the sequence.  (The first
  element must, of course, be such an antecedent.)
\end{definition}

\begin{definition}{Meaning of a program P, $\bf{M^{prf}(P)}$}\\
$M^{prf}(P)$ is the set containing exactly every element $d \in D$ for
  which there exists a proof sequence for $d$ from the operations of
  $P$.
\end{definition}

Now we should prove that $M^{prf}(P) ~=~ M^{bu}(P) ~=~ M^{least}(P)$.
We will leave that to the reader.  It is not very difficult and is a
good exercise to become familiar with these definitions.



===============

\begin{definition}{$T_P$ Operator}
  For program $P$, $T_P$ is an operator from sets of trees to sets of
  trees, as follows:
  
  $T_P(S)~=~\bigcup_{R \in P}T_R(S)$
\end{definition}

\begin{definition}{Semantics of Inductive Programs, $M_P$}
The meaning of a program $P$ is the least fixed point of the operator $T_P$.
\end{definition}

This is well defined, since $T_P$ is a monotonic operator and so is
known to have a least fixed point.

Exercise to prove if $T_P$ has a fixed point, it has a (unique) least
fixed point.

\begin{example}
Given the program $P$:
\begin{verbatim}
nil.
cons(aa,Tail) :- Tail.
\end{verbatim}
the set
\begin{verbatim}
  {nil, cons(aa,nil), cons(aa,cons(aa,nil)),
    cons(aa,cons(aa,cons(aa,nil))), ...}
\end{verbatim}
is a fixed point of $T_P$, since the first rule generates {\tt nil},
and the second rule generates for each element in the set, the element
following it.  Thus all the elements of the set are generated and so
this set is a fixed point of $T_P$.  One can see that there is no
smaller set that is also a fixed point.
\end{example}

\section{Evaluation of Inductive Programs}

We provide two different definitions of the least fixed point of the
operator $T_P$, which can be used to compute elements of the fixed
point: a) the union of iterations of the program operator, $T_P$, and
b) a proof that a element is in the set.  We leave the proof that
these are equivalent to the least fixed point definition as an
exercise for the reader.

\begin{definition}{Iterated Operator Application}
  Given a set of trees $S$, the iterated application of $T_P$ to $S$,
  $T_P^i(S)$, is defined as follows: \\
  $T_P^1(S)~=~T_P(S)$ \\
  $T_P^{i+1}(S)~=~T_P(T_P^i(S))$
\end{definition}

\begin{definition}{Iterated fixed point of $T_P$, $M_P^{Iter}$}
  For program $P$, the iterated fixed point of $T_P$, $M_P^{Iter}$, is
  defined as follows:

  $M_P^{Iter}~=~\bigcup_{i=1}^\infty T_P^i(\emptyset)$
\end{definition}

\begin{example}
Consider the program $P$:
\begin{verbatim}
nil.
cons(aa,Tail) :- Tail.
\end{verbatim}
Then
$T_P^1(\emptyset)~=~\{nil\}$\\
$T_P^2(\emptyset)~=~T_P(\{nil\})~=~\{nil,cons(aa,nil)\}$\\
$T_P^3(\emptyset)~=~T_P(\{nil,cons(aa,nil)\})~=~\{nil,cons(aa,nil),cons(aa,cons(aa,nil))\}$\\
$...$\\
So we can see that $M_P^{Iter}$, being the union of these sets, is the set
\begin{verbatim}
  {nil, cons(aa,nil), cons(aa,cons(aa,nil)),
    cons(aa,cons(aa,cons(aa,nil))), ...}
\end{verbatim}
\end{example}

Next we define a proof of a tree from a program, $P$.  

\begin{definition}{Proof from program $P$}
  A {\em proof of tree $t_n$ from program $P$} is a sequence of trees,
  $t_0, t_1, t_2, ..., t_n$ such that for every tree $t_i$ in the
  sequence, there are trees $t_{i_1}$, $t_{i_2}$, ..., $t_{i_k}$ in
  the sequence preceding $t_i$ such that $t_i$ :- $t_{i_1}$, $t_{i_2}$,
  ..., $t_{i_k}$ is a ground instance of a rule $R \in P$.
\end{definition}

\begin{definition}{Trees provable from program $P$, $M_P^{Prf}$}
$M_P^{Prf}~=~\{t$ : There is a proof of $t$ from program $P\}$, i.e.,
  the set of all trees for which there is a proof from $P$.
\end{definition}

\begin{example}
Again consider the program $P$:
\begin{verbatim}
a. nil.
b. cons(aa,Tail) :- Tail.
\end{verbatim}
(with rules labeled for reference)
and a proof of \verb|cons(aa,cons(aa,cons(aa,nil)))| from $P$:
\begin{verbatim}
1. nil
2. cons(aa,nil)
3. cons(aa,cons(aa,nil))
4. cons(aa,cons(aa,cons(aa,nil)))
\end{verbatim}
with the trees of the proof sequence numbered for reference.
Tree \verb|1.| is justified by rule \verb|a.|, which has
no body and so does not require any trees earlier in the proof.
Tree \verb|2.| is justified by
\verb|cons(aa,nil) :- nil|, a ground instance of rule \verb|b.|,
whose body, \verb|nil|, is tree \verb|1.| in the proof.  
Tree \verb|3.| is justified by
\verb|cons(aa,cons(aa,nil)) :- cons(aa,nil)|, a ground instance of rule \verb|b.|,
whose body, \verb|cons(aa,nil)|, is tree \verb|2.| in the proof.  
Tree \verb|4.| is justified by
\verb|cons(aa,cons(aa,cons(aa,nil))) :- cons(aa,cons(aa,nil))|,
a ground instance of rule \verb|b.|,
whose body, \verb|cons(aa,cons(aa,nil))|, is tree \verb|3.| in the proof.
Thus every tree in the sequence is justified by an instance of a
program rule, so the sequence is a proof.
\end{example}

Note that from any proof, we can easily extract a proof for any tree
that appears in the proof simply by deleting the trees that follow
it.

\begin{theorem}
For every program $P$, $M_P~=~M_P^{Iter}~=~M_P^{Prf}$.
\end{theorem}

\begin{proof}
  Exercise for the reader.
\end{proof}

A few examples:

\begin{example}
We can define the natural numbers with the following program:
\begin{verbatim}
  0.
  s(X) :- X.
\end{verbatim}
where s(X) is the successor of X.  So the natural number $n$ is
represented by a linear tree of $n$ internal nodes labeled with $s$
and the single leaf of $0$.

Then we can define addition of natural numbers.  We'll represent an
addition fact of, say, 1+2=3, with the tree
\verb|plus(s(0),s(s(0)),s(s(s(0))))|.
We want the subtrees of a plus tree to be restricted to trees that
represent natural numbers.  So we will define a set of (linear) trees
each of whose roots is labeled by {\tt nat}, whose child is a natural
number tree (as defined above.)

Then the {\tt plus} program is:
\begin{verbatim}
n1.  nat(0).
n2.  nat(s(X)) :- nat(X).

p1.  plus(0,M,M) :- nat(M).
p2.  plus(s(N),M,s(NM)) :- plus(N,M,NM).
\end{verbatim}
Here we have defined the {\tt nat} trees with the first two rules, and
then {\tt plus} with the last two rules.  The third rule, which builds
a {\tt plus} tree for adding $0$, constrains the second (and third)
subtree to be a natural number subtree.  (We label the rules to allow
us to easily refer them them.)
\end{example}

\begin{example}
We can use this program to define another set of trees, of the form
\verb|mult(N1,N2,N3)|, where N1, N2, and N3 stand for subtrees
representing natural numbers $n_1$, $n_2$, and $n_3$ such that
$n_1*n_2=n_3$.

\begin{verbatim}
m1.  mult(0,M,0) :- nat(M).
m2.  mult(s(N),M,NM) :- mult(N,M,NMProd), plus(M,NMProd,NM).
\end{verbatim}
\end{example}

\subsection{Top-Down Computation}
Let's try to determine whether
\verb|plus(s(s(0)),s(s(0)),s(s(s(s(0)))))| is in $M_P$, where $P$ is
the program including rules {\tt p1}, {\tt p2}, {\tt n1}, and {\tt
  n2}.  How might we go about automatically determining this?  We have
two possible approaches: use the iterative fixed point definition or
use the definition of proofs.  We can try to use either; (Maybe do
iterative first?  But then want a different example maybe.)  Let's
start with the proof approach.

Say we're given the \verb|plus| tree above; how might be go about
trying to construct a proof for this tree?  We can try to construct a
proof by starting with the last element of the proof, which we know
must be this tree, and then trying to add enough elements earlier in
the sequence to construct a valid proof.  So for this example, a proof
we desire must have the following form:
\begin{verbatim}
...
...
plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
\end{verbatim}
I,e., it must be some sequence of trees whose last element is the tree
whose membership in $M_P$ we are trying to determine.  This final tree
must be ``justified'' in the proof by a rule and previous trees.  The
only rule that could justify this term is rule {\tt p2}.  And the
required instance of that rule must be:
\begin{verbatim}
plus(s(s(0)),s(s(0)),s(s(s(s(0))))) :- plus(s(0),s(s(0)),s(s(s(0)))).
\end{verbatim}
The substitution that makes the head of the rule match the tree to be
justified determines this instance.  So we can extend our potential
proof to look like:
\begin{verbatim}
...
plus(s(0),s(s(0)),s(s(s(0))))
plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
\end{verbatim}
Now the last tree is justified, and since {\em all} trees in a proof
must be justified, we now have to turn to justifying the new tree we
just added.  Again, the only rule that can justify the second-to-last
tree is rule {\tt p2} with instance:
\begin{verbatim}
plus(s(0),s(s(0)),s(s(s(0)))) :- plus(0,s(s(0)),s(s(0))).
\end{verbatim}
which allows us to extend our potential partial proof to be:
\begin{verbatim}
...
plus(0,s(s(0)),s(s(0)))
plus(s(0),s(s(0)),s(s(s(0))))
plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
\end{verbatim}
Next we need to justify that third-to-last tree we just added.  The
only rule that could have this term as an instance of its head is rule
{\tt p1}, and it has instance:
\begin{verbatim}
plus(0,s(s(0)),s(s(0))) :- nat(s(s(0))).
\end{verbatim}
so we extend our potential partial proof to:
\begin{verbatim}
...
nat(s(s(0)))
plus(0,s(s(0)),s(s(0)))
plus(s(0),s(s(0)),s(s(s(0))))
plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
\end{verbatim}
Now we need to justify \verb|nat(s(s(0))|.  This can be done only with
rule {\tt n2}, giving us:
\begin{verbatim}
...
nat(s(0))
nat(s(s(0)))
plus(0,s(s(0)),s(s(0)))
plus(s(0),s(s(0)),s(s(s(0))))
plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
\end{verbatim}
and using the {\tt n2} rule again, we get:
\begin{verbatim}
...
nat(0)
nat(s(0))
nat(s(s(0)))
plus(0,s(s(0)),s(s(0)))
plus(s(0),s(s(0)),s(s(s(0))))
plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
\end{verbatim}
And now we can justify \verb|nat(0)| using rule (or actually fact)
{\tt n1}.  But this fact doesn't require any additional tree to be
added to our potential proof in order to be justified.  Now since
every tree is justified by a rule instance whose body trees are
earlier in the sequence, this sequence itself, with no additional
trees, is a proof.  Therefore we've succeeded in constructing a proof
of \verb|plus(s(s(0)),s(s(0)),s(s(s(s(0)))))|; it is:
\begin{verbatim}
nat(0)
nat(s(0))
nat(s(s(0)))
plus(0,s(s(0)),s(s(0)))
plus(s(0),s(s(0)),s(s(s(0))))
plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
\end{verbatim}
And thus we've established that the tree
\verb|plus(s(s(0)),s(s(0)),s(s(s(s(0)))))| $\in P$.

This example has many nice properties that a more complex program and
query tree might not enjoy.  We now ask whether we extend this
computational idea of ``backward proof construction'' to handle those
more complicated situations.

The first generalization requires us to handle rules with more than
one term in their bodies.  In that case we must add all the body trees
to our potential proof and justify all of them in turn.  This just
requires that we keep track of the set of trees that remain to be
justified.  For example, consider the definition:
\begin{verbatim}
1.  mathReq :- discreteMath, calc.
2.  discreteMath :- setTheory, logic.
3.  calc :- calc1, calc2, calc3.

4.  setTheory.
5.  logic.
6.  calc1.
7.  linearAlg.
8.  calc2.
9.  calc3.
\end{verbatim}
whose rules 1 through 3 define the courses required to satisfy a set
of math requirements, and whose facts 6 through 9 define the courses
taken by a particular student.  If {\tt mathReq} is in $M_P$ for this
definition $P$, then the student has satisfied the math requirements
for the major.

We try to generate a proof of {\tt mathReq} from these rules.  Note
that the rules have multiple body terms (basic trees), so we will need
to keep track of which we ones we have started processing to find a
justification.  We will mark the trees in our developing proof with a
``*'' if a potential justification for it has not yet been started,
but will need to be found.  We start with:
\begin{verbatim}
  ...
  *mathReq
\end{verbatim}
The first step is to posit a proof that ends with the tree {\tt
  mathReq}, which needs to be justified and so is marked with an
``*''.  So we find a rule (rule 1) that might justify {\tt mathReq}
and extend the potential proof:
\begin{verbatim}
  ...
  *discreteMath
  *calc
  mathReq
\end{verbatim}
We remove the ``*'' from {\tt mathReq} since its justification has now
been started, and we added the body trees of rule 1 to the potential
proof, with ``*'''s indicating they need to be justified.  Next we
choose the earliest tree in the proof with an ``*'' to justify.  Since
we want to try to justify body trees in left-to-right order, we added
them to the potential proof in reverse order to their appearance in
the body of the rule.  So accordingly, we next choose {\tt
  discreteMath} to expand for justification, getting:
\begin{verbatim}
  ...
  *setTheory
  *logic
  discreteMath
  *calc
  mathReq
\end{verbatim}
Next, choosing {\tt setTheory}, we have that as a fact 4, so that adds
no new trees to justify, and we get:
\begin{verbatim}
  ...
  setTheory
  *logic
  discreteMath
  *calc
  mathReq
\end{verbatim}
Now {\tt logic} is the first non-justified term, so we choose it to
justify.  Again we have the fact 5, and so our partial proof becomes:
\begin{verbatim}
  ...
  setTheory
  logic
  discreteMath
  *calc
  mathReq
\end{verbatim}
Next {\tt calc} requires justification, so we use rule 3, and obtain:
\begin{verbatim}
  ...
  *calc1
  *calc2
  *calc3
  setTheory
  logic
  discreteMath
  calc
  mathReq
\end{verbatim}
Now {\tt calc1} is the next to justify, and with the fact 6, we get:
\begin{verbatim}
  ...
  calc1
  *calc2
  *calc3
  setTheory
  logic
  discreteMath
  calc
  mathReq
\end{verbatim}
And then choosing {\\ calc2} and then {\tt calc3}, with the facts 8
and 9 we eventually get:
\begin{verbatim}
  calc1
  calc2
  calc3
  setTheory
  logic
  discreteMath
  calc
  mathReq
\end{verbatim}
Since no tree has an ``*'', we have fully justified all of them, and
thus we have a proof of {\tt mathReq} in $P$.

The other program complication we have to account for is when we have
more than one rule that might be used to justify a particular tree.
For example, consider the following extension of our requirements
program:

\begin{verbatim}
1.  mathReq :- discreteMath, calc.
2.  discreteMath :- setTheory, logic.
3.  discreteMath :- setTheory, linearAlg.
4.  calc : honorsCalc1, honorsCalc2.
5.  calc :- calc1, calc2, calc3.

6.  setTheory.
7.  honorsCalc1.
8.  calc1.
9.  linearAlg.
10. calc2.
11. calc3.
\end{verbatim}

We have added alternative ways to satisfy the {\tt discreteMath} and
{\tt calc} requirements, and have provided facts for a different
student, who satisfies the requirements differently.  Now we want to
justify {\tt mathReq} for this student.

At each point that we choose a rule to use to justify tree, we might
have multiple rules that could be chosen.  We will try them in order,
choosing the first (or next) and remembering the number of the next
rule to try if this one fails to produce a justification.

We begin with:

\begin{verbatim}
  ...
  *mathReq
\end{verbatim}

We use rule 1 to get:
\begin{verbatim}
  ...
  *discreteMath
  *calc
  mathReq
\end{verbatim}
Since there is only the one rule that might justify {\tt mathReq}, we
don't have to remember any alternative rule.  Next we try to justify
{\tt discreteMath}, and we use its first rule to move to:
\begin{verbatim}
  ...
  *setTheory
  *logic
  (3)discreteMath
  *calc
  mathReq
\end{verbatim}
Here {\tt discreteMath} is in the process of being justified, so we
have removed its ``*'', but there is another rule that might be
applied to justify it, so we record this next rule number with it.
Then we proceed as usual to justify {\tt setTheory}, getting:
\begin{verbatim}
  ...
  setTheory
  *logic
  (3)discreteMath
  *calc
  mathReq
\end{verbatim}
And now we want to justify {\tt logic}.  But there is no fact (or
rule) for this tree, and so there is no justification for {\tt
  logic}.  So this is called ``failure'' and we must try another
alternative.  So we will find the first tree (in our potential partial
proof) that has a unused alternative, i.e., a rule number
associated with it.  Here it is {\tt discreteMath}, and the rule
number is 3.  So we remove every tree back to this numbered tree,
change the number to the next alternative (if there is one, and
deleting it if none).  So here we get:
\begin{verbatim}
  ...
  *setTheory
  *LinearAlg
  discreteMath
  *calc
  mathReq
\end{verbatim}
Then we can justify {\tt setTheory} with fact 6, and {\tt LinearAlg}
with fact 9, yielding:
\begin{verbatim}
  ...
  setTheory
  LinearAlg
  discreteMath
  *calc
  mathReq
\end{verbatim}
Then next we need to justify {\tt calc}, which has two rules, so using
the first rule, 4, and recording the second, we get:
\begin{verbatim}
  ...
  *honorsCalc1
  *honorsCalc2
  setTheory
  LinearAlg
  discreteMath
  (5)calc
  mathReq
\end{verbatim}
Using the fact 7, we get:
\begin{verbatim}
  ...
  honorsCalc1
  *honorsCalc2
  setTheory
  LinearAlg
  discreteMath
  (5)calc
  mathReq
\end{verbatim}
Now again, we have no rule or fact that could justify {\tt
  honorsCalc2}, so we must fail and try an alternative, now using the
top numbered tree, here {\tt (5)calc}, getting:
\begin{verbatim}
  ...
  honorsCalc1
  *honorsCalc2
  setTheory
  LinearAlg
  discreteMath
  (5)calc
  mathReq
\end{verbatim}
OOPS.  When go back to remove star to begin justification, must move
formerly starred tree to top of stack...


A nice property of our example is that at each point in trying to
construct our proof, there was exactly one rule that could generate an
instance whose head matched the tree we were trying to justify.  More
generally, it may be that there may be any (finite) number of rules
(and rule instances) whose heads might match the tree to be
justified.  If there are {\em no} rule instances with a matching head
tree, then that tree cannot be justified, and the current potential
partial proof cannot be extended to a proof.  If there are more than
one instance that can match the tree to be justified, we must try all
of them in turn if we want to find a proof (or all proofs).  Since
some partial proofs may not be extensible to a full proof, as we just
noted, we need to search through all the possible proofs to be sure to
find one if one exists.

(DSW: Do an example that shows these complications, and how to manage them...)


multiple rule bodies; non-determinism.



\subsection{Lazy Grounding in Top-Down Computation}

[Comment DSW: Make proofs in traditional order, forward.  To introduce
  computation, start with propositional program and motivate backwards
  chaining, goal-directed computation.  Then do plus example (no
  existential variables) with ground call.  Then do plus example
  asking for output; now must ``guess'' the grounding, so introduce
  lazy grounding.  Then do example with mult, showing even with ground
  call that existential variables requires lazy grounding.
  Then think about generalizing lazy grounding to full unification...
  ]



We can use either $M_P^{Iter}$ or $M_P^{Prf}$ as the basis for a
computation mechanism for determining membership of trees in a set
determined by a program $P$, that is answers for queries to $P$.

Using $M_P^{Iter}$ leads to a computation strategy known as
``bottom-up evaluation'', whereas $M_P^{Prf}$ leads to a strategy
known as ``top-down evaluation''.

We will develop the top-down strategy.  The idea is to start with a
query, consisting of a sequence of trees.  We try to construct a proof
for every tree in that sequence.  We start constructing a proof by
first taking the query sequence as the initial sequence of our
prospective proof.  We take another copy of the query sequence to
actively process as follows as we add to our proof.  We process the
active query sequence by taking the first tree and finding a rule that
has a ground instance with that tree as its head.  We add the trees of
body of the rule instance to our growing proof, and we replace the
tree in the active query sequence with the sequence of trees that make
up the body of the grounded rule, producing a new active query
sequence.  Then we continue in this same way to process every tree in
the active query sequence, and continue until its list of trees
becomes empty.  If we reach the empty list of terms, we have
accumulated a proof of the first tree in the initial query sequence.
In fact we have a proof of every tree in the proof sequence,
obtainable by deleting all trees preceding that tree in the larger
sequence.

\begin{example}
  Given $P$:
\begin{verbatim}
1.  nat(0).
2.  nat(s(X)) :- nat(X).

3.  plus(0,M,M) :- nat(M).
4.  plus(s(N),M,s(NM)) :- plus(N,M,NM).

5.  mult(0,M,0) :- nat(M).
6.  mult(s(N),M,NM) :- mult(N,M,NMProd), plus(M,NMProd,NM).
\end{verbatim}
and we construct the following  proof of
\verb|mult(s(s(0)),s(s(0)),s(s(s(s(0)))))| from $P$:
\begin{verbatim}
a) mult(s(s(0)),s(s(0)),s(s(s(s(0)))))
b) mult(s(0),s(s(0)),s(s(0))),
c) plus(s(s(0)),s(s(0)),s(s(s(s(0)))))
d) mult(0,s(s(0)),0),
e) plus(s(s(0)),0,s(s(0)))
f) nat(s(s(0)))
g) nat(s(0))
h) nat(0)
i) plus(s(0),0,s(0))
j) plus(0,0,0)
k) nat(0)
l) plus(s(0),s(s(0)),s(s(s(0))))
m) plus(0,s(s(0)),s(s(0)))
n) nat(s(s(0)))
o) nat(s(0))
p) nat(0)
\end{verbatim}
Consider how this sequence can be built.  Our query is
\verb|mult(s(s(0)),s(s(0)),s(s(s(s(0)))))|.  So a proof of this tree
must use rule 6 since that's the only one that has a ground instance
with the term as head.  So we choose the substitution of
\verb|{N->s(0), M->s(s(0)), NM->s(s(s(0))), NMProd->s(s(0))}|, which
when applied to rule 6 gives a ground instance of that rule with the
query as head.  So we add the two ground body terms, b) and c), to our
(potential) proof and then must extend the proof to prove each of
them.  To prove b) we again take an instance of rule 6 using
substitution \verb|{N->0, M->s(s(0)), NM->0, NMProd->0}|.  This adds
the trees d) and e) to our potential proof.  Then we choose tree d) to
prove, and use substitution \verb|{M->s(s(0))}| applied to rule 5 to
get a ground instance with tree d) as head.  So we add its body to our
growing proof as tree f).  Then rule 2 with substitution
\verb|{X->s(0)}| leads to proof element g), and the same rule with
substitution \verb|{X->0}| generates a rule instance with head g) and
body h), which is added to the proof.  h) is a fact in the program:
rule 1, so it doesn't generate anything further that needs proof.  So
we next go back to prove e): \verb|plus(s(s(0)),0,s(s(0))}|.  We
generate a ground instance of rule 4, and generate proof element i).
Then i) leads to j) using rule 4 again, and j) to k) using rule 2.
Now we can go back to item c) that still needs a proof, and an
instance of rule 4 leads to adding item l) to the proof, which leads
to m), which leads to n), which leads to o), which leads to p).  At
this point every item in the proof has been used to generated a rule
instance, with it as head and whose body trees have been added to (and
thus appear in) the proof following the head tree.  Thus we have a
complete proof of the initial query tree.
\end{example}

We see that we have constructed a proof for this query in this case,
but we have done this by hand, choosing rules and choosing ground
substitutions that generate a proof.  Is this a general algorithm that
could be programmed to automatically find a proof of an arbitrary
query from an arbitrary program, whenever one exists?

We must organize the search for a proof, choosing the right rules to
instantiate and the right substitutions that generate desired ground
instances of the rules.  In our example above, we always choose the
right rule and the right instantiation that led to a proof.  Choosing
the right rule was easy in this case since we can see that in every
case, there was exactly one rule whose head could be instantiated to
match the desired tree.  But this won't always be the case for
arbitrary rules and queries.  Sometimes there will be no rules and
sometimes there will be several.  If there are no rules, then we
cannot extend the current potential proof into a full complete proof.
So we must fail the current proof attempt.  If there are several
rules, we must try them all in turn, since any one (or more) of them
might lead to a proof.  And we will want to find all the proofs for a
query.  It may, of course, be the case that multiple rules will be
tried, but after more computation we find that no rule matches a
subsequent tree, in which case that proof attempt fails, but one that
used a different rule, or substitution, might lead to a full roof.  So
we must search them all.  With careful organization, this can be done.

However, there is a more difficult problem when we look at the problem
of choosing ''the right'' ground substitution after we've chosen the
rule to match.  In many cases, we can see that only one substitution
will work, and there is no problem.  For example when we wanted to
prove \verb|nat(s(0))| in our example, the only choice for a
substitution when we use rule 2 is \verb|{X=0}| and applying that
substitution to the rule generates a ground instance.  However, if we
look at trying to prove the original query,
\verb|mult(s(s(0)),s(s(0)),s(s(s(s(0)))))|, we see that only rule 6
can be used and by matching the query to the head of the rule, we can
determine the partial substitution
\verb|{N->s(0), M->s(s(0)), NM->s(s(s(s(0))))}|, but we don't know
what the value for the variable \verb|NMProd| should be; it is not
determined by the matching rule head.  For the proof above, since we
know what the trees rooted by \verb|mult| and \verb|add| are intended
to mean, we can, with that knowledge, guess the right instance, but in
general, for arbitrary programs, we can't assume an algorithm will
have our insight.  We might just guess every possibility, trying each
in turn.  But there are infinitely many possible trees that could
work, so that is not feasible.

In fact, this problem might arise even for queries to \verb|plus|
trees.  Our example query above was ground, but in general we will
want to be able to evaluate a query such as
\verb|plus(s(0),s(s(0)),X)|, where we ask for instances of the query,
i.e., for values of the variable \verb|X|, in this case to be told
the value of $1 + 2$.  Note that in this case we aren't given a value
for \verb|X| in the query; we want the evaluation strategy to
determine it.  Can this be done?

The answer is yes and the idea is to do ``lazy grounding''.  That is,
if we can't determine what the value for a variable is, we leave it as
a variable and wait until later in the computation, when value is
determined.  So rather than computing with trees (i.e., ground terms),
we generalize the computation to work with general terms, which may
contain variables.  Then rather than require ground instances at each
step in our proof construction, we allow general terms.  We can think
of the computation as maintaining the {\em set} of trees that are the
possible ground instances of the?????

WORK ON PROOF FORM TO SHOW SUBSTITUTIONS AND RULES AS ANNOTATIONS

all possible
substitutions 
Each step in
our computation can be thought of as further





(Then ask how do we know what instances to choose.  Do matching of
heads to query, but this doesn't ground all of them.  What to do with
them?  Do lazy grounding; only ground as much as necessary, and leave
the others as variables to be grounded later when required.
Do the example and show.  See how much of unification is needed.  Try
to see if can get away with just matching here (I hope.)  Then
introduce case where full unification is needed....


\begin{definition}{Query and Answer}
  A {\em query} is a non-empty sequence of terms.  An {\em answer for
    a query $Q$ to program $P$} is a substitution $theta$ for the
  variables in $Q$ such that for every tree $t \in Q$, $t\theta \in
  M_P$.  (DSW: this is only for *an* answer; and what about ground
  queries?  Put this defn later, after example of ground query and
  answer of true or false. And after introducing variables in
  queries.)
\end{definition}





\section{Conclusion}
if any...
